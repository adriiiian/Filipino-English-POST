{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b4b7d",
   "metadata": {},
   "source": [
    "# Testing Different Monolingual Filipino and English Part of Speech (POS) Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62008160",
   "metadata": {},
   "source": [
    "### PLEASE TAKE NOTE!!!\n",
    "- [IMPORTANT] Always refresh kernel, clear outputs, and save before exiting to avoid git conflicts\n",
    "- Current formatting of this .ipynb is not final, will reformat when testing sample data from FilWordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9a8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lingua import Language, LanguageDetectorBuilder #used for language identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbc41b",
   "metadata": {},
   "source": [
    "Initialize language Identification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebb1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935269c",
   "metadata": {},
   "source": [
    "Initialize the dataframe that will hold the sentences and its pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e071b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_format = {\n",
    "    \"text\": [],\n",
    "    \"general_tags\": [],\n",
    "    \"specific_tags\": [],\n",
    "    \"token_tagset\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d488a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>general_tags</th>\n",
       "      <th>specific_tags</th>\n",
       "      <th>token_tagset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, general_tags, specific_tags, token_tagset]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>general_tags</th>\n",
       "      <th>specific_tags</th>\n",
       "      <th>token_tagset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, general_tags, specific_tags, token_tagset]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>general_tags</th>\n",
       "      <th>specific_tags</th>\n",
       "      <th>token_tagset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, general_tags, specific_tags, token_tagset]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>general_tags</th>\n",
       "      <th>specific_tags</th>\n",
       "      <th>token_tagset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, general_tags, specific_tags, token_tagset]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tagged_texts_combi1_ff = pd.DataFrame(df_format)\n",
    "tagged_texts_combi2_ff = pd.DataFrame(df_format)\n",
    "\n",
    "tagged_texts_combi1_sf = pd.DataFrame(df_format)\n",
    "tagged_texts_combi2_sf = pd.DataFrame(df_format)\n",
    "\n",
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi2_ff)\n",
    "\n",
    "display(tagged_texts_combi1_sf)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c92704",
   "metadata": {},
   "source": [
    "## Loading the test data\n",
    "\n",
    "Let us load the .json input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcf2ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'token': 'Nakakababa', 'tag': 'JJ'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'IQ', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'fanaticism', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'maling', 'tag': 'JJ'}</td>\n",
       "      <td>{'token': 'tao', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': '.', 'tag': 'PUNC'}</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'token': 'Maraming', 'tag': 'JJ'}</td>\n",
       "      <td>{'token': 'mga', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'oldies', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'doon', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'hirap', 'tag': 'JJ'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'trabaho', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'pero', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': '``', 'tag': 'SYM'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'token': 'Naaalala', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'ko', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'pa', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'naman', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'mga', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'pinaggagagawa', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'ko', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'nung', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'araw', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'CCP'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'token': 'Pwedeng', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'galing', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'inyo', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'o', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'mga', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'kakilala', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'ninyo', 'tag': 'PR'}</td>\n",
       "      <td>{'token': '.', 'tag': 'PUNC'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'token': 'Pero', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'nauubos', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'CCP'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'pera', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'namin', 'tag': 'PR'}</td>\n",
       "      <td>{'token': '.', 'tag': 'PUNC'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>{'token': 'Hindi', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'muna', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'siya', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'gagawa', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'movie', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'at', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'magko-concentrate', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'muna', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CONJ'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>{'token': 'Sa', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'lakas', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'pagkakabangga', 'tag': 'VB'}</td>\n",
       "      <td>{'token': ',', 'tag': 'PUNC'}</td>\n",
       "      <td>{'token': 'nawasak', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'unang', 'tag': 'CD'}</td>\n",
       "      <td>{'token': 'bahagi', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>{'token': 'Kinilala', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'lamang', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'napatay', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'CCP'}</td>\n",
       "      <td>{'token': 'suspek', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'sa', 'tag': 'CCP'}</td>\n",
       "      <td>{'token': 'alyas', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'nito', 'tag': 'PR'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'CCP'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>{'token': 'Sabi', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'raw', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'tumawag', 'tag': 'VB'}</td>\n",
       "      <td>{'token': ',', 'tag': 'PUNC'}</td>\n",
       "      <td>{'token': 'kulang', 'tag': 'JJ'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'pera', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'kaniyang', 'tag': 'PR'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>{'token': 'Tinuntunton', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': 'na', 'tag': 'RB'}</td>\n",
       "      <td>{'token': 'ng', 'tag': 'CONJ'}</td>\n",
       "      <td>{'token': 'mga', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'awtoridad', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': ',', 'tag': 'PUNC'}</td>\n",
       "      <td>{'token': 'ang', 'tag': 'DT'}</td>\n",
       "      <td>{'token': 'nakasagasang', 'tag': 'VB'}</td>\n",
       "      <td>{'token': 'motorsiklo', 'tag': 'NOUN'}</td>\n",
       "      <td>{'token': '.', 'tag': 'PUNC'}</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1472 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0    \\\n",
       "0        {'token': 'Nakakababa', 'tag': 'JJ'}   \n",
       "1          {'token': 'Maraming', 'tag': 'JJ'}   \n",
       "2          {'token': 'Naaalala', 'tag': 'VB'}   \n",
       "3           {'token': 'Pwedeng', 'tag': 'RB'}   \n",
       "4            {'token': 'Pero', 'tag': 'CONJ'}   \n",
       "...                                       ...   \n",
       "1467          {'token': 'Hindi', 'tag': 'RB'}   \n",
       "1468           {'token': 'Sa', 'tag': 'CONJ'}   \n",
       "1469       {'token': 'Kinilala', 'tag': 'VB'}   \n",
       "1470           {'token': 'Sabi', 'tag': 'VB'}   \n",
       "1471  {'token': 'Tinuntunton', 'tag': 'NOUN'}   \n",
       "\n",
       "                                    1                                   2    \\\n",
       "0        {'token': 'ng', 'tag': 'CONJ'}      {'token': 'IQ', 'tag': 'NOUN'}   \n",
       "1         {'token': 'mga', 'tag': 'DT'}  {'token': 'oldies', 'tag': 'NOUN'}   \n",
       "2          {'token': 'ko', 'tag': 'PR'}        {'token': 'pa', 'tag': 'RB'}   \n",
       "3      {'token': 'galing', 'tag': 'RB'}      {'token': 'sa', 'tag': 'CONJ'}   \n",
       "4     {'token': 'nauubos', 'tag': 'VB'}       {'token': 'na', 'tag': 'CCP'}   \n",
       "...                                 ...                                 ...   \n",
       "1467     {'token': 'muna', 'tag': 'RB'}      {'token': 'siya', 'tag': 'PR'}   \n",
       "1468  {'token': 'lakas', 'tag': 'NOUN'}      {'token': 'ng', 'tag': 'CONJ'}   \n",
       "1469   {'token': 'lamang', 'tag': 'RB'}       {'token': 'ang', 'tag': 'DT'}   \n",
       "1470      {'token': 'raw', 'tag': 'RB'}      {'token': 'ng', 'tag': 'CONJ'}   \n",
       "1471       {'token': 'na', 'tag': 'RB'}      {'token': 'ng', 'tag': 'CONJ'}   \n",
       "\n",
       "                                          3    \\\n",
       "0               {'token': 'ang', 'tag': 'DT'}   \n",
       "1              {'token': 'doon', 'tag': 'PR'}   \n",
       "2             {'token': 'naman', 'tag': 'RB'}   \n",
       "3              {'token': 'inyo', 'tag': 'PR'}   \n",
       "4               {'token': 'ang', 'tag': 'DT'}   \n",
       "...                                       ...   \n",
       "1467         {'token': 'gagawa', 'tag': 'VB'}   \n",
       "1468  {'token': 'pagkakabangga', 'tag': 'VB'}   \n",
       "1469        {'token': 'napatay', 'tag': 'VB'}   \n",
       "1470        {'token': 'tumawag', 'tag': 'VB'}   \n",
       "1471            {'token': 'mga', 'tag': 'DT'}   \n",
       "\n",
       "                                         4    \\\n",
       "0     {'token': 'fanaticism', 'tag': 'NOUN'}   \n",
       "1             {'token': 'na', 'tag': 'CONJ'}   \n",
       "2              {'token': 'mga', 'tag': 'DT'}   \n",
       "3              {'token': 'o', 'tag': 'CONJ'}   \n",
       "4           {'token': 'pera', 'tag': 'NOUN'}   \n",
       "...                                      ...   \n",
       "1467          {'token': 'ng', 'tag': 'CONJ'}   \n",
       "1468           {'token': ',', 'tag': 'PUNC'}   \n",
       "1469           {'token': 'na', 'tag': 'CCP'}   \n",
       "1470           {'token': ',', 'tag': 'PUNC'}   \n",
       "1471   {'token': 'awtoridad', 'tag': 'NOUN'}   \n",
       "\n",
       "                                          5    \\\n",
       "0              {'token': 'sa', 'tag': 'CONJ'}   \n",
       "1             {'token': 'hirap', 'tag': 'JJ'}   \n",
       "2     {'token': 'pinaggagagawa', 'tag': 'VB'}   \n",
       "3              {'token': 'sa', 'tag': 'CONJ'}   \n",
       "4             {'token': 'namin', 'tag': 'PR'}   \n",
       "...                                       ...   \n",
       "1467        {'token': 'movie', 'tag': 'NOUN'}   \n",
       "1468        {'token': 'nawasak', 'tag': 'VB'}   \n",
       "1469       {'token': 'suspek', 'tag': 'NOUN'}   \n",
       "1470         {'token': 'kulang', 'tag': 'JJ'}   \n",
       "1471            {'token': ',', 'tag': 'PUNC'}   \n",
       "\n",
       "                                   6    \\\n",
       "0     {'token': 'maling', 'tag': 'JJ'}   \n",
       "1       {'token': 'sa', 'tag': 'CONJ'}   \n",
       "2         {'token': 'ko', 'tag': 'PR'}   \n",
       "3        {'token': 'mga', 'tag': 'DT'}   \n",
       "4        {'token': '.', 'tag': 'PUNC'}   \n",
       "...                                ...   \n",
       "1467    {'token': 'at', 'tag': 'CONJ'}   \n",
       "1468     {'token': 'ang', 'tag': 'DT'}   \n",
       "1469     {'token': 'sa', 'tag': 'CCP'}   \n",
       "1470     {'token': 'ang', 'tag': 'DT'}   \n",
       "1471     {'token': 'ang', 'tag': 'DT'}   \n",
       "\n",
       "                                              7    \\\n",
       "0                 {'token': 'tao', 'tag': 'NOUN'}   \n",
       "1             {'token': 'trabaho', 'tag': 'NOUN'}   \n",
       "2                  {'token': 'nung', 'tag': 'RB'}   \n",
       "3            {'token': 'kakilala', 'tag': 'NOUN'}   \n",
       "4                                            None   \n",
       "...                                           ...   \n",
       "1467  {'token': 'magko-concentrate', 'tag': 'VB'}   \n",
       "1468              {'token': 'unang', 'tag': 'CD'}   \n",
       "1469            {'token': 'alyas', 'tag': 'NOUN'}   \n",
       "1470             {'token': 'pera', 'tag': 'NOUN'}   \n",
       "1471       {'token': 'nakasagasang', 'tag': 'VB'}   \n",
       "\n",
       "                                         8    \\\n",
       "0              {'token': '.', 'tag': 'PUNC'}   \n",
       "1           {'token': 'pero', 'tag': 'CONJ'}   \n",
       "2           {'token': 'araw', 'tag': 'NOUN'}   \n",
       "3            {'token': 'ninyo', 'tag': 'PR'}   \n",
       "4                                       None   \n",
       "...                                      ...   \n",
       "1467          {'token': 'muna', 'tag': 'RB'}   \n",
       "1468      {'token': 'bahagi', 'tag': 'NOUN'}   \n",
       "1469          {'token': 'nito', 'tag': 'PR'}   \n",
       "1470          {'token': 'ng', 'tag': 'CONJ'}   \n",
       "1471  {'token': 'motorsiklo', 'tag': 'NOUN'}   \n",
       "\n",
       "                                     9    ...   202   203   204   205   206  \\\n",
       "0                                   None  ...  None  None  None  None  None   \n",
       "1          {'token': '``', 'tag': 'SYM'}  ...  None  None  None  None  None   \n",
       "2          {'token': 'na', 'tag': 'CCP'}  ...  None  None  None  None  None   \n",
       "3          {'token': '.', 'tag': 'PUNC'}  ...  None  None  None  None  None   \n",
       "4                                   None  ...  None  None  None  None  None   \n",
       "...                                  ...  ...   ...   ...   ...   ...   ...   \n",
       "1467      {'token': 'sa', 'tag': 'CONJ'}  ...  None  None  None  None  None   \n",
       "1468      {'token': 'ng', 'tag': 'CONJ'}  ...  None  None  None  None  None   \n",
       "1469       {'token': 'na', 'tag': 'CCP'}  ...  None  None  None  None  None   \n",
       "1470  {'token': 'kaniyang', 'tag': 'PR'}  ...  None  None  None  None  None   \n",
       "1471       {'token': '.', 'tag': 'PUNC'}  ...  None  None  None  None  None   \n",
       "\n",
       "       207   208   209   210   211  \n",
       "0     None  None  None  None  None  \n",
       "1     None  None  None  None  None  \n",
       "2     None  None  None  None  None  \n",
       "3     None  None  None  None  None  \n",
       "4     None  None  None  None  None  \n",
       "...    ...   ...   ...   ...   ...  \n",
       "1467  None  None  None  None  None  \n",
       "1468  None  None  None  None  None  \n",
       "1469  None  None  None  None  None  \n",
       "1470  None  None  None  None  None  \n",
       "1471  None  None  None  None  None  \n",
       "\n",
       "[1472 rows x 212 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_dataframe = pd.read_json(\"input/homographtext.json\")\n",
    "# input_dataframe = pd.read_json(\"input/englishmonolingualtext.json\")\n",
    "# input_dataframe = pd.read_json(\"input/filipinomonolingualtext.json\")\n",
    "# input_dataframe = pd.read_json(\"input/intersentential.json\")\n",
    "# input_dataframe = pd.read_json(\"input/intraword.json\")\n",
    "# input_dataframe = pd.read_json(\"input/verb_tenses.json\")\n",
    "input_dataframe = pd.read_json(\"input/input_data_validated_cleaned_v2.json\")\n",
    "display(input_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e57ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_temp = []\n",
    "tags_temp = []\n",
    "input_sentence = []\n",
    "\n",
    "for i in range(len(input_dataframe)):\n",
    "    tokens_temp.clear()\n",
    "    tags_temp.clear()\n",
    "    \n",
    "    for j in range(input_dataframe.iloc[i].count()):\n",
    "        tokens_temp.append(input_dataframe.iloc[i][j].__getitem__(\"token\"))\n",
    "        tags_temp.append(input_dataframe.iloc[i][j].__getitem__(\"tag\"))\n",
    "        \n",
    "    sentence_temp = ' '.join([str(item) for item in tokens_temp])\n",
    "    \n",
    "    input_sentence.append(sentence_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70e96d",
   "metadata": {},
   "source": [
    "## POS TAGGERS\n",
    "\n",
    "Let us import the monolingual taggers. Flair for english pos tagger and FSPOST for filipino pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd55134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-29 20:54:32,231 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "# FLAIR POS TAGGER\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "flair_tagger = SequenceTagger.load(\"flair/pos-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3df152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY POS TAGGER\n",
    "import spacy\n",
    "\n",
    "spacy_tagger = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSPOST POS TAGGER\n",
    "import os\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.internals import find_jars_within_path\n",
    "\n",
    "# These are Windows formatted directories\n",
    "model = 'model//filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "jar = 'lib//stanford-postagger.jar'\n",
    "\n",
    "# These are Linux formatted directories\n",
    "# model = 'model/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger'\n",
    "# jar = 'lib/stanford-postagger.jar'\n",
    "\n",
    "fspost = StanfordPOSTagger(model, path_to_jar=jar)  # Load Tagger Model\n",
    "fspost._SEPARATOR = '|'  # Set separator for proper tuple formatting (word, tag)\n",
    "\n",
    "# print(fspost._stanford_jar)\n",
    "\n",
    "# stanford_dir = fspost._stanford_jar.rpartition('/')[0]\n",
    "# stanford_jars = find_jars_within_path(stanford_dir)\n",
    "\n",
    "# fspost._stanford_jar = ':'.join(stanford_jars)\n",
    "\n",
    "def set_java_path(file_path):\n",
    "    \"\"\"\n",
    "    Function for setting java path to make Stanford POS Tagger work. Makes use of the 'os' library. Input \"\" to use\n",
    "    default java path, otherwise set the location.\n",
    "    Args:\n",
    "        file_path (str): The java file path / location.\n",
    "    \"\"\"\n",
    "    if file_path == \"\":\n",
    "        java_path = \"C:/Program Files/Java/jdk1.8.0_111/bin/java.exe\"\n",
    "        print(\"Java path set by default\")\n",
    "    else:\n",
    "        java_path = file_path\n",
    "        print(\"Java path set from given\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "def tag_string(sentence):\n",
    "    \"\"\"\n",
    "    Function for tagging a sentence/string. Output is a (word, pos) tuple. To output a POS-only string, enclose this\n",
    "    function with 'format_pos' function. Ex. fspost.format_pos(fspost.tag_string('this is a string')). Same goes for\n",
    "    Stanford's word|tag notation, use 'format_stanford' function.\n",
    "    Args:\n",
    "        sentence (str): The string to be tagged.\n",
    "    Returns:\n",
    "        tagged_string: a list of string tokens containing POS labeled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()  # Tokenize Sentence by whitespaces\n",
    "    tagged_string = fspost.tag(tokens)\n",
    "    return tagged_string\n",
    "\n",
    "def tag_string_list(sentence_list):\n",
    "    \"\"\"\n",
    "    Function for tagging a list of sentences. Output is a list of (word, pos) tuple. To output a POS-only string,\n",
    "    enclose the elements in this function with 'format_pos' function. Same goes for Stanford's word|tag notation, use\n",
    "    'format_stanford' function.\n",
    "    Args:\n",
    "        sentence_list (list): The list of strings to be tagged.\n",
    "    Returns:\n",
    "        tagged_list: a list of strings containing POS labelled (word, pos) tuples.\n",
    "    \"\"\"\n",
    "    progress_ctr = 0\n",
    "    tagged_list = []  # Initialize an empty list\n",
    "    for sentence in sentence_list:\n",
    "        tagged_tuple = tag_string(sentence)  # Tag each sentence in the list\n",
    "        tagged_list.append(tagged_tuple)  # Insert tagged sentence in the new list\n",
    "        progress_ctr += 1\n",
    "        print(progress_ctr, \"/\", len(sentence_list))  # Progress Counter\n",
    "    return tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fba20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java path set from given\n"
     ]
    }
   ],
   "source": [
    "# WINDOWS\n",
    "# set_java_path(\"C:/Users/Adrian Jerez/Desktop/THESIS REPO/java-11-openjdk/bin/\")\n",
    "set_java_path(\"C:/Program Files/Java/jdk-19/bin/\")\n",
    "\n",
    "\n",
    "# LINUX\n",
    "# set_java_path(\"/usr/lib/jvm/java-11-openjdk-amd64/bin/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355279da",
   "metadata": {},
   "source": [
    "### Create functions to be used for POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea912637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the eng POS tag (Flair version)\n",
    "def eng_tagger_flair(input_string):\n",
    "    sentence = Sentence(input_string)\n",
    "    flair_tagger.predict(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a14812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_tagger_spacy(input_string):\n",
    "    return spacy_tagger(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb5edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the fil POS tag\n",
    "def fil_tagger(input_string):\n",
    "    return tag_string(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e21d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the specific tag into generalized tag\n",
    "def convert_eng(pos_tag):\n",
    "    if(pos_tag == \"NN\" or pos_tag == \"NNS\"):\n",
    "        return \"NOUN\"\n",
    "    elif(pos_tag == \"NNP\" or pos_tag == \"NNPS\"):\n",
    "        return \"PROPN\"\n",
    "    elif(pos_tag == \"PRP\" or pos_tag == \"PRP$\" or pos_tag == \"WP\" or pos_tag == \"WP$\"):\n",
    "        return \"PR\"\n",
    "    elif(pos_tag == \"DT\" or pos_tag == \"WDT\"):\n",
    "        return \"DT\"\n",
    "    elif(pos_tag == \"CC\"):\n",
    "        return \"CONJ\"\n",
    "    elif(pos_tag == \"IN\"):\n",
    "        return \"IN\"\n",
    "    elif(pos_tag == \"VB\"):\n",
    "        return \"VB\"\n",
    "    elif(pos_tag == \"VBD\" or pos_tag == \"VBN\"):\n",
    "        return \"VBPT\"\n",
    "    elif(pos_tag == \"VBP\" or pos_tag == \"VBZ\" or pos_tag == \"VBG\"):\n",
    "        return \"VBPR\"\n",
    "    elif(pos_tag == \"JJ\" or pos_tag == \"JJR\" or pos_tag == \"JJS\"):\n",
    "        return \"JJ\"\n",
    "    elif(pos_tag == \"CD\"):\n",
    "        return \"CD\"\n",
    "    elif(pos_tag == \"RB\" or pos_tag == \"RBR\" or pos_tag == \"RBS\" or pos_tag == \"WRB\" or pos_tag == \"RP\"):\n",
    "        return \"RB\"\n",
    "    elif(pos_tag == \"UH\"):\n",
    "        return \"UH\"\n",
    "    elif(pos_tag == \"FW\"):\n",
    "        return \"FW\"\n",
    "    elif(pos_tag == \".\" or pos_tag == \",\" or pos_tag == \":\" or pos_tag == \"NFP\" or pos_tag == \"(\" or pos_tag == \")\"\n",
    "        or pos_tag == \"''\" or pos_tag == '\"\"' or pos_tag == \"``\" or pos_tag == \"`\" or pos_tag == \"-RRB-\"\n",
    "        or pos_tag == \"-LRB-\"):\n",
    "        return \"PUNC\"\n",
    "    elif(pos_tag == \"HYPH\" or pos_tag == \"SYM\" or pos_tag == \"$\" or pos_tag == \"\\\"\" or pos_tag == \"LS\"):\n",
    "        return \"SYM\"\n",
    "    else:\n",
    "        return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04781d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the specific tag into generalized tag\n",
    "def convert_fil(pos_tag):\n",
    "    if(pos_tag == \"NNC\" or pos_tag == \"NNCA\"):\n",
    "        return \"NOUN\"\n",
    "    elif(pos_tag == \"NNP\" or pos_tag == \"NNPA\"):\n",
    "        return \"PROPN\"\n",
    "    elif(pos_tag == \"PRS\" or pos_tag == \"PRP\" or pos_tag == \"PRSP\" or pos_tag == \"PRO\"\n",
    "        or pos_tag == \"PRQ\" or pos_tag == \"PRQP\" or pos_tag == \"PRL\" or pos_tag == \"PRC\"\n",
    "        or pos_tag == \"PRF\" or pos_tag == \"PRI\"):\n",
    "        return \"PR\"\n",
    "    elif(pos_tag == \"DTC\" or pos_tag == \"DTCP\" or pos_tag == \"DTP\" or pos_tag == \"DTPP\"):\n",
    "        return \"DT\"\n",
    "    elif(pos_tag == \"LM\"):\n",
    "        return \"LM\"\n",
    "    elif(pos_tag == \"CCT\" or pos_tag == \"CCR\" or pos_tag == \"CCB\" or pos_tag == \"CCA\"):\n",
    "        return \"CONJ\"\n",
    "    elif(pos_tag == \"CCP\"):\n",
    "        return \"CCP\"\n",
    "    elif(pos_tag == \"CCU\"):\n",
    "        return \"IN\"\n",
    "    elif(pos_tag == \"VBW\" or pos_tag == \"VBS\" or pos_tag == \"VBH\" or pos_tag == \"VBN\"\n",
    "        or pos_tag == \"VBAF\" or pos_tag == \"VBOF\" or pos_tag == \"VBOB\" or pos_tag == \"VBOL\"\n",
    "        or pos_tag == \"VBOI\" or pos_tag == \"VBRF\"):\n",
    "        return \"VB\"\n",
    "    elif(pos_tag == \"VBTS\" or pos_tag == \"VBTP\"):\n",
    "        return \"VBPT\"\n",
    "    elif(pos_tag == \"VBTR\"):\n",
    "        return \"VBPR\"\n",
    "    elif(pos_tag == \"VBTF\"):\n",
    "        return \"VBFT\"\n",
    "    elif(pos_tag == \"JJD\" or pos_tag == \"JJC\" or pos_tag == \"JJCC\" or pos_tag == \"JJCS\" or pos_tag == \"JJCN\"):\n",
    "        return \"JJ\"\n",
    "    elif(pos_tag == \"JJN\" or pos_tag == \"CDB\"):\n",
    "        return \"CD\"\n",
    "    elif(pos_tag == \"RBD\" or pos_tag == \"RBN\" or pos_tag == \"RBK\" or pos_tag == \"RBP\"\n",
    "        or pos_tag == \"RBB\" or pos_tag == \"RBR\" or pos_tag == \"RBQ\" or pos_tag == \"RBT\"\n",
    "        or pos_tag == \"RBF\" or pos_tag == \"RBW\" or pos_tag == \"RBM\" or pos_tag == \"RBL\"\n",
    "        or pos_tag == \"RBI\" or pos_tag == \"RBS\"):\n",
    "        return \"RB\"\n",
    "    elif(pos_tag == \"RBJ\"):\n",
    "        return \"UH\"\n",
    "    elif(pos_tag == \"FW\"):\n",
    "        return \"FW\"\n",
    "    elif(pos_tag == \"PMP\" or pos_tag == \"PME\" or pos_tag == \"PMQ\" or pos_tag == \"PMC\" or pos_tag == \"PMSC\"):\n",
    "        return \"PUNC\"\n",
    "    elif(pos_tag == \"PMS\"):\n",
    "        return \"SYM\"\n",
    "    else:\n",
    "        return pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e61c509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langid = [] # will be used to store language id\n",
    "\n",
    "def reset_variables(general, specific, token_tagset):\n",
    "    #langid.clear()\n",
    "    general.clear()\n",
    "    specific.clear()\n",
    "    token_tagset.clear()\n",
    "    return\n",
    "\n",
    "def reset_variables_combi2(gen_eng, spec_eng, gen_fil, spec_fil):\n",
    "    gen_eng.clear()\n",
    "    spec_eng.clear()\n",
    "    gen_fil.clear()\n",
    "    spec_fil.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "456baed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to be used for Flair-FSPOST Tagger\n",
    "\n",
    "pos_tags_general_ff = [] # will be used to store generalized pos tags\n",
    "pos_tags_specific_ff = [] # will be used to store specific pos tags\n",
    "token_tagset_ff = [] # will be used to store the name of the tagset used for specific tags\n",
    "\n",
    "# Temporary lists to be used for combi 2\n",
    "pos_tags_general_eng_ff = []\n",
    "pos_tags_specific_eng_ff = []\n",
    "pos_tags_general_fil_ff = []\n",
    "pos_tags_specific_fil_ff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "854669e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to be used for Spacy-FSPOST Tagger\n",
    "\n",
    "pos_tags_general_sf = [] # will be used to store generalized pos tags\n",
    "pos_tags_specific_sf = [] # will be used to store specific pos tags\n",
    "token_tagset_sf = [] # will be used to store the name of the tagset used for specific tags\n",
    "\n",
    "# Temporary lists to be used for combi 2\n",
    "pos_tags_general_eng_sf = []\n",
    "pos_tags_specific_eng_sf = []\n",
    "pos_tags_general_fil_sf = []\n",
    "pos_tags_specific_fil_sf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a2d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dataframe(input_sentence, general_tags, specific_tags, tagset, tagged_texts):\n",
    "    tagged_texts = pd.concat([tagged_texts, pd.DataFrame.from_records([{\"text\": input_sentence,\n",
    "                            \"general_tags\": np.array(general_tags), \"specific_tags\": np.array(specific_tags),\n",
    "                            \"token_tagset\": np.array(tagset)}])], ignore_index = True)\n",
    "    return tagged_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c8077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_string(string_list):\n",
    "    return ' '.join([str(item) for item in string_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c41330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9eb2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_text_with_punc(input_text):\n",
    "    input_text_tokenized = nltk.word_tokenize(input_text)\n",
    "    return input_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "421acd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_per_token(text_wo_punc):\n",
    "    langid = []\n",
    "    #Identifies the language of each tokens to determine which tagger to use\n",
    "    for i in range(len(text_wo_punc)):\n",
    "        langid.append(detector.detect_language_of(text_wo_punc[i]))\n",
    "        \n",
    "    return langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8672b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_csv(dataframes, output_name):\n",
    "    dataframes['general_tags'] = dataframes['general_tags'].map(list)\n",
    "    dataframes['specific_tags'] = dataframes['specific_tags'].map(list)\n",
    "    dataframes['token_tagset'] = dataframes['token_tagset'].map(list)\n",
    "    dataframes.to_csv(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e28df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMultipleTags(tag):\n",
    "    if(tag.__contains__('_')):\n",
    "        new_tag = tag.split('_')\n",
    "        return new_tag[0]\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc923d",
   "metadata": {},
   "source": [
    "## Language Identification then Monolingual Tagging (COMBI 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "832f8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_id_then_mono_tag(input_string):\n",
    "    \n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # reset temporary variables\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff) # Flair-FSPOST version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf) # Spacy-FSPOST version\n",
    "    \n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "\n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            token = eng_tagger_flair(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token.get_labels('pos')[0].value)\n",
    "\n",
    "            if(new_token != \"''\"):\n",
    "                pos_tags_general_ff.append(convert_eng(new_token))\n",
    "                pos_tags_specific_ff.append(new_token)\n",
    "                token_tagset_ff.append(\"Flair\")\n",
    "            else:\n",
    "                new_token = isMultipleTags(token.get_labels('pos')[1].value)\n",
    "                pos_tags_general_ff.append(convert_eng(new_token))\n",
    "                pos_tags_specific_ff.append(new_token)\n",
    "                token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "            \n",
    "             # Spacy-FSPOST dataframes\n",
    "            token = eng_tagger_spacy(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0].tag_)\n",
    "\n",
    "            if(new_token != \"``\"):\n",
    "                pos_tags_general_sf.append(convert_eng(new_token))\n",
    "                pos_tags_specific_sf.append(new_token)\n",
    "                token_tagset_sf.append(\"Spacy\")\n",
    "            else:\n",
    "                new_token = isMultipleTags(token[1].tag_)\n",
    "                pos_tags_general_sf.append(convert_eng(new_token))\n",
    "                pos_tags_specific_sf.append(new_token)\n",
    "                token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "\n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "    global tagged_texts_combi1_ff\n",
    "    temp = tagged_texts_combi1_ff\n",
    "    tagged_texts_combi1_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi1_sf\n",
    "    temp = tagged_texts_combi1_sf\n",
    "    tagged_texts_combi1_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                                 pos_tags_specific_sf, token_tagset_sf, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1c795ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass  0\n",
      "pass  1\n",
      "pass  2\n",
      "pass  3\n",
      "pass  4\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    lang_id_then_mono_tag(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi1_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_id_then_mono_tag(\"Kung baga , it may be touched at some points of the course pero hindi talaga sya major major thing 'no ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4be3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(eng_tagger_flair(\"Kung baga , it may be touched at some points of the course pero hindi talaga sya major major thing 'no ?\"))\n",
    "# test = eng_tagger_flair(\"Kung baga , it may be touched at some points of the course pero hindi talaga sya major major thing 'no ?\")\n",
    "# for i in range(len(test.get_labels('pos'))):\n",
    "#     print(test.get_token(i + 1), ' : ', test.get_labels('pos')[i].value, ' : ', convert_eng(test.get_labels('pos')[i].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2 = eng_tagger_spacy(\"'no\")\n",
    "# # for i in range(len(test2)):\n",
    "# #     print(test2[i], ' : ', test2[i].tag_, ' : ', convert_eng(test2[i].tag_))\n",
    "\n",
    "# print(test2[1], ' : ', test2[1].tag_, ' : ', convert_eng(test2[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68321ce3",
   "metadata": {},
   "source": [
    "## Monolingual Tagging then Language Identification (COMBI 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_tag_then_lang_id(input_string):\n",
    "    # Resets temp variables FLair version\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff)\n",
    "    reset_variables_combi2(pos_tags_general_eng_ff, pos_tags_specific_eng_ff,\n",
    "                           pos_tags_general_fil_ff, pos_tags_specific_fil_ff)\n",
    "    \n",
    "    # Resets temp variables Spacy version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf)\n",
    "    reset_variables_combi2(pos_tags_general_eng_sf, pos_tags_specific_eng_sf,\n",
    "                           pos_tags_general_fil_sf, pos_tags_specific_fil_sf)\n",
    "    \n",
    "    # Tokenized sentences\n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # Flair (English) pos tagging\n",
    "    token_eng_flair = eng_tagger_flair(input_string)\n",
    "    # Spacy (English) pos tagging\n",
    "    token_eng_spacy = eng_tagger_spacy(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (ENGLISH - Flair Tagger)\n",
    "    for i in range(len(token_eng_flair.get_labels('pos'))):\n",
    "        new_token = isMultipleTags(token_eng_flair.get_labels('pos')[i].value)\n",
    "        if(new_token != \"''\"):\n",
    "            pos_tags_general_eng_ff.append(convert_eng(new_token))\n",
    "            pos_tags_specific_eng_ff.append(new_token)\n",
    "        \n",
    "   # Store tags to temporary variables (ENGLISH - Spacy Tagger)\n",
    "    for i in range(len(token_eng_spacy)):\n",
    "        new_token = isMultipleTags(token_eng_spacy[i].tag_)\n",
    "        if(new_token != \"``\"):\n",
    "            pos_tags_general_eng_sf.append(convert_eng(new_token))\n",
    "            pos_tags_specific_eng_sf.append(new_token)\n",
    "         \n",
    "        \n",
    "    # FSPOST (Filipino) pos tagging\n",
    "    token_fil = fil_tagger(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (FILIPINO Tagger)\n",
    "    for i in range(len(token_fil)):\n",
    "        new_token = isMultipleTags(token_fil[i][1])\n",
    "        \n",
    "        # Store filipino tags on Flair-FSPOST dataframe\n",
    "        pos_tags_general_fil_ff.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_ff.append(new_token)\n",
    "        \n",
    "        # Store filipino tags on Spacy-FSPOST dataframe\n",
    "        pos_tags_general_fil_sf.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_sf.append(new_token)\n",
    "    \n",
    "    # Get Languages per token ( Language Identification )\n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            pos_tags_general_ff.append(pos_tags_general_eng_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_eng_ff[i])\n",
    "            token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_eng_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_eng_sf[i])\n",
    "            token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "        \n",
    "    global tagged_texts_combi2_ff\n",
    "    temp = tagged_texts_combi2_ff\n",
    "    tagged_texts_combi2_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi2_sf\n",
    "    temp = tagged_texts_combi2_sf\n",
    "    tagged_texts_combi2_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                              pos_tags_specific_sf, token_tagset_sf, temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e39498",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    mono_tag_then_lang_id(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "        \n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_csv(tagged_texts_combi1_ff, \"output/full_data/Flair-FSPOST-Combination-1-full_data.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_ff, \"output/full_data/Flair-FSPOST-Combination-2-full_data.csv\")\n",
    "\n",
    "dataframe_to_csv(tagged_texts_combi1_sf, \"output/full_data/Spacy-FSPOST-Combination-1-full_data.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_sf, \"output/full_data/Spacy-FSPOST-Combination-2-full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ad85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi1_sf)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed55716",
   "metadata": {},
   "source": [
    "## Results evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df461cbb",
   "metadata": {},
   "source": [
    "Reading the output csv files of each combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c806a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_output = pd.read_csv(\"Flair-FSPOST-Combination-1-demo_0626.csv\")\n",
    "ff_combi2_output = pd.read_csv(\"Flair-FSPOST-Combination-2-demo_0626.csv\")\n",
    "sf_combi1_output = pd.read_csv(\"Spacy-FSPOST-Combination-1-demo_0626.csv\")\n",
    "sf_combi2_output = pd.read_csv(\"Spacy-FSPOST-Combination-2-demo_0626.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3546072",
   "metadata": {},
   "source": [
    "### Declaring functions to be used for results evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_columns(output):\n",
    "    output['general_tags'] = output['general_tags'].apply(eval)\n",
    "    output['specific_tags'] = output['specific_tags'].apply(eval)\n",
    "    output['token_tagset'] = output['token_tagset'].apply(eval)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1D(series):\n",
    "    return pd.Series([x for _list in series for x in _list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db483b09",
   "metadata": {},
   "source": [
    "### Taking the number of each POS tags per combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_output = fixed_columns(ff_combi1_output)\n",
    "ff_combi2_output = fixed_columns(ff_combi2_output)\n",
    "sf_combi1_output = fixed_columns(sf_combi1_output)\n",
    "sf_combi2_output = fixed_columns(sf_combi2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_tag_counts = to_1D(ff_combi1_output['general_tags']).value_counts()\n",
    "ff_combi1_total = ff_combi1_tag_counts.sum()\n",
    "\n",
    "ff_combi2_tag_counts = to_1D(ff_combi2_output['general_tags']).value_counts()\n",
    "ff_combi2_total = ff_combi2_tag_counts.sum()\n",
    "\n",
    "sf_combi1_tag_counts = to_1D(sf_combi1_output['general_tags']).value_counts()\n",
    "sf_combi1_total = sf_combi1_tag_counts.sum()\n",
    "\n",
    "sf_combi2_tag_counts = to_1D(sf_combi2_output['general_tags']).value_counts()\n",
    "sf_combi2_total = sf_combi2_tag_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tag_counts(tag_counts, total, tagger):\n",
    "    print(tag_counts)\n",
    "    print(tagger, \" total tokens: \", total, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tag_counts(ff_combi1_tag_counts, ff_combi1_total, \"Flair-FSPOST Combi1\")\n",
    "print_tag_counts(ff_combi2_tag_counts, ff_combi2_total, \"Flair-FSPOST Combi2\")\n",
    "print_tag_counts(sf_combi1_tag_counts, sf_combi1_total, \"Spacy-FSPOST Combi1\")\n",
    "print_tag_counts(sf_combi2_tag_counts, sf_combi2_total, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644bbd8",
   "metadata": {},
   "source": [
    "### Taking overall accuracy of each combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee86b3e",
   "metadata": {},
   "source": [
    "Let us first read the correct tags from our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_temp = []\n",
    "tags_list = []\n",
    "\n",
    "for i in range(len(input_dataframe)):\n",
    "    pos_temp.clear()\n",
    "    \n",
    "    for j in range(input_dataframe.iloc[i].count()):\n",
    "        pos_temp.append(input_dataframe.iloc[i][j].__getitem__(\"tag\"))\n",
    "    \n",
    "    temp = np.array(pos_temp)\n",
    "    tags_list.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538c9b4",
   "metadata": {},
   "source": [
    "Let us print the number of tags present in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_counts = to_1D(tags_list).value_counts()\n",
    "test_data_counts_total = test_data_counts.sum()\n",
    "\n",
    "print_tag_counts(test_data_counts, test_data_counts_total, \"Test data counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977ef08",
   "metadata": {},
   "source": [
    "Almost all POS tags in our tagset is present in our test data. Although ADD, PDT and AFX are missing in our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dffa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_accuracy(output):\n",
    "    accuracy = []\n",
    "    for i in range(len(tags_list)):\n",
    "        counter = 0\n",
    "        for j in range(len(tags_list[i])):\n",
    "            if tags_list[i][j] == 'VB':\n",
    "                if (output['general_tags'][i][j] == 'VB' or output['general_tags'][i][j] == 'VBPT' or\n",
    "                output['general_tags'][i][j] == 'VBPR' or output['general_tags'][i][j] == 'VBFT'):\n",
    "                    counter = counter + 1\n",
    "            elif tags_list[i][j] == output['general_tags'][i][j]:\n",
    "                counter = counter + 1\n",
    "                \n",
    "        accuracy.append(counter / len(tags_list[i]))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_overall_accuracy(output, tagger):\n",
    "    accuracy = get_overall_accuracy(output)\n",
    "    print(tagger, ' accuracy: %f' % (sum(accuracy) / len(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_overall_accuracy(ff_combi1_output, \"Flair-FSPOST Combi1\")\n",
    "print_overall_accuracy(sf_combi1_output, \"Spacy-FSPOST Combi1\")\n",
    "print_overall_accuracy(ff_combi2_output, \"Flair-FSPOST Combi2\")\n",
    "print_overall_accuracy(sf_combi2_output, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158fdab9",
   "metadata": {},
   "source": [
    "### Taking each pos tag accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tags = ['NOUN', 'PROPN', 'PR', 'DT', 'LM', 'CONJ', 'CCP', 'IN', 'VB', 'JJ', 'CD', 'RB', 'UH',\n",
    "                  'TS', 'FW', 'PUNC', 'SYM', 'EX', 'TO', 'ADD', 'POS', 'PDT', 'XX', 'MD', 'AFX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_per_tags(output, tag):\n",
    "    counter_right = 0\n",
    "    counter_total = 0\n",
    "    for i in range(len(tags_list)):\n",
    "        for j in range(len(tags_list[i])):\n",
    "            if tags_list[i][j] == tag:\n",
    "                counter_total = counter_total + 1\n",
    "                if tags_list[i][j] == 'VB':\n",
    "                    if (output['general_tags'][i][j] == 'VB' or output['general_tags'][i][j] == 'VBPT' or\n",
    "                    output['general_tags'][i][j] == 'VBPR' or output['general_tags'][i][j] == 'VBFT'):\n",
    "                        counter_right = counter_right + 1\n",
    "                elif tags_list[i][j] == output['general_tags'][i][j]:\n",
    "                    counter_right = counter_right + 1\n",
    "    \n",
    "    if counter_total == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return counter_right / counter_total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_per_tags(output, tagger):\n",
    "    print(\"Tagger: \", tagger)\n",
    "    for i in range(len(possible_tags)):\n",
    "        accuracy = get_accuracy_per_tags(output, possible_tags[i])\n",
    "        if accuracy != None:\n",
    "            print('POS Tag: ', possible_tags[i], ' accuracy: %f' % accuracy)\n",
    "            \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32901a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_accuracy_per_tags(ff_combi1_output, \"Flair-FSPOST Combi1\")\n",
    "print_accuracy_per_tags(sf_combi1_output, \"Spacy-FSPOST Combi1\")\n",
    "print_accuracy_per_tags(ff_combi2_output, \"Flair-FSPOST Combi2\")\n",
    "print_accuracy_per_tags(sf_combi2_output, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ba98f",
   "metadata": {},
   "source": [
    "### Generating confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tags = []\n",
    "predicted_tags = []\n",
    "for i in range(len(tags_list)):\n",
    "    for j in range(len(tags_list[i])):\n",
    "        actual_tags.append(tags_list[i][j])\n",
    "        if (ff_combi1_output['general_tags'][i][j] == 'VB' or ff_combi1_output['general_tags'][i][j] == 'VBPT' or\n",
    "                ff_combi1_output['general_tags'][i][j] == 'VBPR' or ff_combi1_output['general_tags'][i][j] == 'VBFT'):\n",
    "            predicted_tags.append('VB')\n",
    "        else:\n",
    "            predicted_tags.append(ff_combi1_output['general_tags'][i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_pos_tags = ['NOUN', 'PROPN', 'PR', 'DT', 'LM', 'CONJ', 'CCP', 'IN', 'VB', 'JJ', 'CD', 'RB', 'UH',\n",
    "#                  'TS', 'FW', 'PUNC', 'SYM', 'EX', 'TO', 'POS', 'XX', 'MD']\n",
    "\n",
    "dataset_pos_tags = ['NOUN', 'CONJ', 'VB', 'PR', 'JJ', 'RB', 'PUNC', 'DT', 'CCP', 'PROPN', 'MD', 'UH', 'CD', 'TO', 'LM', 'IN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349c51f",
   "metadata": {},
   "source": [
    "Taking the actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6884cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(label):\n",
    "    return str(label).replace(\"['\", '').replace(\"']\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c914146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(combi):\n",
    "    actual_tags = []\n",
    "    predicted_tags = []\n",
    "    for i in range(len(tags_list)):\n",
    "        for j in range(len(tags_list[i])):\n",
    "            actual_tags.append(tags_list[i][j])\n",
    "            if (combi['general_tags'][i][j] == 'VB' or combi['general_tags'][i][j] == 'VBPT' or\n",
    "                    combi['general_tags'][i][j] == 'VBPR' or combi['general_tags'][i][j] == 'VBFT'):\n",
    "                predicted_tags.append('VB')\n",
    "            else:\n",
    "                predicted_tags.append(combi['general_tags'][i][j])\n",
    "            \n",
    "    np.seterr(invalid='ignore')\n",
    "    plt.figure(figsize=(60, 40))\n",
    "    plt.rcParams.update({'font.size': 32})\n",
    "    cm = metrics.confusion_matrix(actual_tags, predicted_tags, labels=dataset_pos_tags, normalize='true')\n",
    "    cm_df = pd.DataFrame(cm, columns=dataset_pos_tags)\n",
    "    order = np.argsort(-cm_df.to_numpy().diagonal())\n",
    "    \n",
    "    label_df = pd.DataFrame(dataset_pos_tags)\n",
    "    label_df = label_df.iloc[order].to_numpy()\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(label_df)):\n",
    "        labels.append(clean_labels(label_df[i]))\n",
    "    \n",
    "    fx = sns.heatmap(cm_df.iloc[order, order], annot=True, fmt=\".2f\", cmap=\"OrRd\")\n",
    "    fx.set_title('Confusion Matrix \\n')\n",
    "    fx.set_xlabel('\\n Predicted Values\\n')\n",
    "    fx.set_ylabel('\\n Actual Values\\n')\n",
    "    fx.xaxis.set_ticklabels(labels)\n",
    "    fx.yaxis.set_ticklabels(labels)\n",
    "    plt.show()\n",
    "    \n",
    "    print(metrics.classification_report(actual_tags, predicted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(ff_combi1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70699458",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(sf_combi1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(ff_combi2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix(sf_combi2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a0ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed664c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ac6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567318c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298f297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574e8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e410cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(tag_list, tag):\n",
    "    tags = []\n",
    "    \n",
    "    for i in range(len(tag_list)):\n",
    "        for j in range(len(tag_list[i])):\n",
    "            if tag_list[i][j] == tag:\n",
    "                tags.append(True)\n",
    "                \n",
    "            else:\n",
    "                tags.append(False)\n",
    "                \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81847f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confu_matrix(combi, top_tags):\n",
    "    actual_tags = []\n",
    "    predicted_tags = []\n",
    "\n",
    "    for i in range(len(top_tags)):\n",
    "        actual_tags = get_tags(tags_list, top_tags[i])\n",
    "        predicted_tags = get_tags(combi['general_tags'], top_tags[i])\n",
    "\n",
    "        confusion_matrix_ff_combi1 = metrics.confusion_matrix(actual_tags, predicted_tags)\n",
    "\n",
    "        cm_display_ff_combi1 = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_ff_combi1, \n",
    "                                                          display_labels = [False, True])\n",
    "\n",
    "        cm_display_ff_combi1.plot()\n",
    "        plt.show()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(actual_tags, predicted_tags)\n",
    "        precision = metrics.precision_score(actual_tags, predicted_tags)\n",
    "        specificity = metrics.recall_score(actual_tags, predicted_tags, pos_label = 0)\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Specificity: \", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e82cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_top_tags = ['CCP', 'NOUN']\n",
    "sf_combi1_top_tags = ['CCP', 'NOUN']\n",
    "ff_combi2_top_tags = ['CCP', 'NOUN']\n",
    "sf_combi2_top_tags = ['CCP', 'NOUN']\n",
    "\n",
    "generate_confu_matrix(ff_combi1_output, ff_combi1_top_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confu_matrix(sf_combi1_output, sf_combi1_top_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05123096",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confu_matrix(ff_combi2_output, ff_combi2_top_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confu_matrix(sf_combi2_output, sf_combi2_top_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a1c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6482fdb",
   "metadata": {},
   "source": [
    "### TO BE DELETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataframe = pd.read_json(\"VB_Tense_batch.json\")\n",
    "display(input_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_temp = []\n",
    "tags_temp = []\n",
    "input_sentence = []\n",
    "\n",
    "for i in range(len(input_dataframe)):\n",
    "    tokens_temp.clear()\n",
    "    tags_temp.clear()\n",
    "    \n",
    "    for j in range(input_dataframe.iloc[i].count()):\n",
    "        tokens_temp.append(input_dataframe.iloc[i][j].__getitem__(\"token\"))\n",
    "        tags_temp.append(input_dataframe.iloc[i][j].__getitem__(\"tag\"))\n",
    "        \n",
    "    sentence_temp = ' '.join([str(item) for item in tokens_temp])\n",
    "    \n",
    "    input_sentence.append(sentence_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c39dd",
   "metadata": {},
   "source": [
    "### COMBI 1 FOR VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_id_then_mono_tag_verbs(input_string):\n",
    "    \n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # reset temporary variables\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff) # Flair-FSPOST version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf) # Spacy-FSPOST version\n",
    "    \n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            token = eng_tagger_flair(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token.get_labels('pos')[0].value)\n",
    "            pos_tags_general_ff.append(convert_eng(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "             # Spacy-FSPOST dataframes\n",
    "            token = eng_tagger_spacy(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0].tag_)\n",
    "            pos_tags_general_sf.append(convert_eng(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            token = fil_tagger(input_text_tokenized[i])\n",
    "            new_token = isMultipleTags(token[0][1])\n",
    "            \n",
    "            # Flair-FSPOST dataframes\n",
    "            pos_tags_general_ff.append(convert_fil(new_token))\n",
    "            pos_tags_specific_ff.append(new_token)\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            # Spacy-FSPOST dataframes\n",
    "            pos_tags_general_sf.append(convert_fil(new_token))\n",
    "            pos_tags_specific_sf.append(new_token)\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "    global tagged_texts_combi1_ff\n",
    "    temp = tagged_texts_combi1_ff\n",
    "    tagged_texts_combi1_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi1_sf\n",
    "    temp = tagged_texts_combi1_sf\n",
    "    tagged_texts_combi1_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                                 pos_tags_specific_sf, token_tagset_sf, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc443543",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    lang_id_then_mono_tag_verbs(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi1_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e795c",
   "metadata": {},
   "source": [
    "### COMBI 2 FOR VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989dda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_tag_then_lang_id_verbs(input_string):\n",
    "    # Resets temp variables FLair version\n",
    "    reset_variables(pos_tags_general_ff, pos_tags_specific_ff, token_tagset_ff)\n",
    "    reset_variables_combi2(pos_tags_general_eng_ff, pos_tags_specific_eng_ff,\n",
    "                           pos_tags_general_fil_ff, pos_tags_specific_fil_ff)\n",
    "    \n",
    "    # Resets temp variables Spacy version\n",
    "    reset_variables(pos_tags_general_sf, pos_tags_specific_sf, token_tagset_sf)\n",
    "    reset_variables_combi2(pos_tags_general_eng_sf, pos_tags_specific_eng_sf,\n",
    "                           pos_tags_general_fil_sf, pos_tags_specific_fil_sf)\n",
    "    \n",
    "    # Tokenized sentences\n",
    "    input_text_tokenized = tokenized_text_with_punc(input_string)\n",
    "    \n",
    "    # Flair (English) pos tagging\n",
    "    token_eng_flair = eng_tagger_flair(input_string)\n",
    "    # Spacy (English) pos tagging\n",
    "    token_eng_spacy = eng_tagger_spacy(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (ENGLISH - Flair Tagger)\n",
    "    for i in range(len(token_eng_flair.get_labels('pos'))):\n",
    "        new_token = isMultipleTags(token_eng_flair.get_labels('pos')[i].value)\n",
    "        pos_tags_general_eng_ff.append(convert_eng(new_token))\n",
    "        pos_tags_specific_eng_ff.append(new_token)\n",
    "        \n",
    "   # Store tags to temporary variables (ENGLISH - Spacy Tagger)\n",
    "    for i in range(len(token_eng_spacy)):\n",
    "        new_token = isMultipleTags(token_eng_spacy[i].tag_)\n",
    "        pos_tags_general_eng_sf.append(convert_eng(new_token))\n",
    "        pos_tags_specific_eng_sf.append(new_token)\n",
    "         \n",
    "        \n",
    "    # FSPOST (Filipino) pos tagging\n",
    "    token_fil = fil_tagger(input_string)\n",
    "    \n",
    "    # Store tags to temporary variables (FILIPINO Tagger)\n",
    "    for i in range(len(token_fil)):\n",
    "        new_token = isMultipleTags(token_fil[i][1])\n",
    "        \n",
    "        # Store filipino tags on Flair-FSPOST dataframe\n",
    "        pos_tags_general_fil_ff.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_ff.append(new_token)\n",
    "        \n",
    "        # Store filipino tags on Spacy-FSPOST dataframe\n",
    "        pos_tags_general_fil_sf.append(convert_fil(new_token))\n",
    "        pos_tags_specific_fil_sf.append(new_token)\n",
    "    \n",
    "    # Get Languages per token ( Language Identification )\n",
    "    langid = get_lang_per_token(input_text_tokenized)\n",
    "    \n",
    "    for i in range(len(input_text_tokenized)):\n",
    "        if(langid[i] == Language.TAGALOG):\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "\n",
    "        elif(langid[i] == Language.ENGLISH):\n",
    "            pos_tags_general_ff.append(pos_tags_general_eng_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_eng_ff[i])\n",
    "            token_tagset_ff.append(\"Flair\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_eng_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_eng_sf[i])\n",
    "            token_tagset_sf.append(\"Spacy\")\n",
    "\n",
    "        else:\n",
    "            pos_tags_general_ff.append(pos_tags_general_fil_ff[i])\n",
    "            pos_tags_specific_ff.append(pos_tags_specific_fil_ff[i])\n",
    "            token_tagset_ff.append(\"MGNN\")\n",
    "            \n",
    "            pos_tags_general_sf.append(pos_tags_general_fil_sf[i])\n",
    "            pos_tags_specific_sf.append(pos_tags_specific_fil_sf[i])\n",
    "            token_tagset_sf.append(\"MGNN\")\n",
    "        \n",
    "        \n",
    "    global tagged_texts_combi2_ff\n",
    "    temp = tagged_texts_combi2_ff\n",
    "    tagged_texts_combi2_ff = append_to_dataframe(input_string, pos_tags_general_ff,\n",
    "                                              pos_tags_specific_ff, token_tagset_ff, temp)\n",
    "    \n",
    "    global tagged_texts_combi2_sf\n",
    "    temp = tagged_texts_combi2_sf\n",
    "    tagged_texts_combi2_sf = append_to_dataframe(input_string, pos_tags_general_sf,\n",
    "                                              pos_tags_specific_sf, token_tagset_sf, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d52c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range(len(input_sentence)):\n",
    "    #try:\n",
    "    mono_tag_then_lang_id_verbs(input_sentence[i])\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"pass \", j)\n",
    "        j = j + 1\n",
    "            \n",
    "    #except:\n",
    "    #print(i, ': ', input_sentence[i])\n",
    "        \n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_csv(tagged_texts_combi1_ff, \"Flair-FSPOST-Combination-1-VERBS.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_ff, \"Flair-FSPOST-Combination-2-VERBS.csv\")\n",
    "\n",
    "dataframe_to_csv(tagged_texts_combi1_sf, \"Spacy-FSPOST-Combination-1-VERBS.csv\")\n",
    "dataframe_to_csv(tagged_texts_combi2_sf, \"Spacy-FSPOST-Combination-2-VERBS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tagged_texts_combi1_ff)\n",
    "display(tagged_texts_combi2_ff)\n",
    "display(tagged_texts_combi1_sf)\n",
    "display(tagged_texts_combi2_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_output = pd.read_csv(\"Flair-FSPOST-Combination-1-VERBS.csv\")\n",
    "ff_combi2_output = pd.read_csv(\"Flair-FSPOST-Combination-2-VERBS.csv\")\n",
    "sf_combi1_output = pd.read_csv(\"Spacy-FSPOST-Combination-1-VERBS.csv\")\n",
    "sf_combi2_output = pd.read_csv(\"Spacy-FSPOST-Combination-2-VERBS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_output = fixed_columns(ff_combi1_output)\n",
    "ff_combi2_output = fixed_columns(ff_combi2_output)\n",
    "sf_combi1_output = fixed_columns(sf_combi1_output)\n",
    "sf_combi2_output = fixed_columns(sf_combi2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_combi1_tag_counts = to_1D(ff_combi1_output['general_tags']).value_counts()\n",
    "ff_combi1_total = ff_combi1_tag_counts.sum()\n",
    "\n",
    "ff_combi2_tag_counts = to_1D(ff_combi2_output['general_tags']).value_counts()\n",
    "ff_combi2_total = ff_combi2_tag_counts.sum()\n",
    "\n",
    "sf_combi1_tag_counts = to_1D(sf_combi1_output['general_tags']).value_counts()\n",
    "sf_combi1_total = sf_combi1_tag_counts.sum()\n",
    "\n",
    "sf_combi2_tag_counts = to_1D(sf_combi2_output['general_tags']).value_counts()\n",
    "sf_combi2_total = sf_combi2_tag_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356cbf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tag_counts(ff_combi1_tag_counts, ff_combi1_total, \"Flair-FSPOST Combi1\")\n",
    "print_tag_counts(ff_combi2_tag_counts, ff_combi2_total, \"Flair-FSPOST Combi2\")\n",
    "print_tag_counts(sf_combi1_tag_counts, sf_combi1_total, \"Spacy-FSPOST Combi1\")\n",
    "print_tag_counts(sf_combi2_tag_counts, sf_combi2_total, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0535bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_temp = []\n",
    "tags_list = []\n",
    "\n",
    "for i in range(len(input_dataframe)):\n",
    "    pos_temp.clear()\n",
    "    \n",
    "    for j in range(input_dataframe.iloc[i].count()):\n",
    "        pos_temp.append(input_dataframe.iloc[i][j].__getitem__(\"tag\"))\n",
    "    \n",
    "    temp = np.array(pos_temp)\n",
    "    tags_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bda8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_counts = to_1D(tags_list).value_counts()\n",
    "test_data_counts_total = test_data_counts.sum()\n",
    "\n",
    "print_tag_counts(test_data_counts, test_data_counts_total, \"Test data counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6333dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_accuracy(output):\n",
    "    accuracy = []\n",
    "    for i in range(len(tags_list)):\n",
    "        counter = 0\n",
    "        for j in range(len(tags_list[i])):\n",
    "            if tags_list[i][j] == output['general_tags'][i][j]:\n",
    "                counter = counter + 1\n",
    "                \n",
    "        accuracy.append(counter / len(tags_list[i]))\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621edbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_overall_accuracy(output, tagger):\n",
    "    accuracy = get_overall_accuracy(output)\n",
    "    print(tagger, ' accuracy: %f' % (sum(accuracy) / len(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_overall_accuracy(ff_combi1_output, \"Flair-FSPOST Combi1\")\n",
    "print_overall_accuracy(sf_combi1_output, \"Spacy-FSPOST Combi1\")\n",
    "print_overall_accuracy(ff_combi2_output, \"Flair-FSPOST Combi2\")\n",
    "print_overall_accuracy(sf_combi2_output, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tags = ['NOUN', 'PROPN', 'PR', 'DT', 'LM', 'CONJ', 'CCP', 'IN', 'VB', 'VBPT', 'VBPR', 'VBFT', 'JJ', 'CD', 'RB', 'UH',\n",
    "                 'TS', 'FW', 'PUNC', 'SYM', 'EX', 'TO', 'ADD', 'POS', 'PDT', 'XX', 'MD', 'AFX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_per_tags(output, tag):\n",
    "    counter_right = 0\n",
    "    counter_total = 0\n",
    "    for i in range(len(tags_list)):\n",
    "        for j in range(len(tags_list[i])):\n",
    "            if tags_list[i][j] == tag:\n",
    "                counter_total = counter_total + 1\n",
    "                if tags_list[i][j] == output['general_tags'][i][j]:\n",
    "                    counter_right = counter_right + 1\n",
    "    \n",
    "    if counter_total == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return counter_right / counter_total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_per_tags(output, tagger):\n",
    "    print(\"Tagger: \", tagger)\n",
    "    for i in range(len(possible_tags)):\n",
    "        accuracy = get_accuracy_per_tags(output, possible_tags[i])\n",
    "        if accuracy != None:\n",
    "            print('POS Tag: ', possible_tags[i], ' accuracy: %f' % accuracy)\n",
    "            \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50118c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy_per_tags(ff_combi1_output, \"Flair-FSPOST Combi1\")\n",
    "print_accuracy_per_tags(sf_combi1_output, \"Spacy-FSPOST Combi1\")\n",
    "print_accuracy_per_tags(ff_combi2_output, \"Flair-FSPOST Combi2\")\n",
    "print_accuracy_per_tags(sf_combi2_output, \"Spacy-FSPOST Combi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71650f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = [\"Alas\", \"dos\", \"na\", \".\"]\n",
    "dump2 = \"Magandang umaga\"\n",
    "# print(tag_string(dump2))\n",
    "print(tag_string(dump2))\n",
    "print(tag_string(\"alas\"))\n",
    "print(tag_string(\"dos\"))\n",
    "print(tag_string(\"na\"))\n",
    "print(tag_string(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c83d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence(\"Ano ang nangyayari Kaguluhan sa at kasamaan dumadaloy sa sistema natinPag kitil ng buhay ganoon nalang ba kadaliAng sigaw ng tulong ng mga kababayan natin kay dali nalang balewalain.\")\n",
    "sentence2 = Sentence(\"ajsdskdskdjs hello there\")\n",
    "print(flair_tagger.predict(sentence2))\n",
    "print(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = Sentence(\"Alas\")\n",
    "sent2 = Sentence(\"dos\")\n",
    "sent3 = Sentence(\"na\")\n",
    "sent4 = Sentence(\".\")\n",
    "print(flair_tagger.predict(sent1))\n",
    "print(flair_tagger.predict(sent2))\n",
    "print(flair_tagger.predict(sent3))\n",
    "print(flair_tagger.predict(sent4))\n",
    "print(sent1)\n",
    "print(sent2)\n",
    "print(sent3)\n",
    "print(sent4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spacy_tagger(\"Ano ang nangyayari Kaguluhan sa at kasamaan dumadaloy sa sistema natinPag kitil ng buhay ganoon nalang ba kadaliAng sigaw ng tulong ng mga kababayan natin kay dali nalang balewalain.\")\n",
    "for i in range(len(temp)):\n",
    "    print(temp[i].tag_, \" : \", temp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = spacy_tagger(\"Alas\")\n",
    "temp2 = spacy_tagger(\"dos\")\n",
    "temp3 = spacy_tagger(\"na\")\n",
    "temp4 = spacy_tagger(\".\")\n",
    "print(temp1[0].tag_)\n",
    "print(temp2[0].tag_)\n",
    "print(temp3[0].tag_)\n",
    "print(temp4[0].tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_id_then_mono_tag(\"So usap ng mga a minute or so nga lang sa tg, and then bam !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagged_texts_combi1_ff['general_tags'])\n",
    "display(tagged_texts_combi1_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the eng POS tag (Flair version)\n",
    "def eng_tagger_flair(input_string):\n",
    "    sentence = Sentence(input_string)\n",
    "    flair_tagger.predict(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_tagger_spacy(input_string):\n",
    "    return spacy_tagger(input_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
